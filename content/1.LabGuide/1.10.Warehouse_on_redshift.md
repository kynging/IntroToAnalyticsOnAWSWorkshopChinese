---
title: "10. Redshift 数据仓库"
chapter: false
weight: 10
---

在本模块中，我们将设置一个 Amazon Redshift 集群，并使用 AWS Glue 将数据加载到 Amazon Redshift。我们将了解有关在 Redshift 中创建数据并将数据加载到表中以及对其运行查询的几个设计注意事项和最佳实践。

![](/images/1.LabGuide/redshift.png)

#### 创建 Redshift IAM 角色

在此步骤中，我们将为 Redshift 集群创建一个 IAM 角色。

- 前往 Redshift 控制台：[https://console.aws.amazon.com/iam/home?region=us-east-1#/roles](https://console.aws.amazon.com/iam/home?region=us-east-1#/roles)
  - 点击**创建角色**
  - 选择 **Redshift**
  - 选择 **Redshift - 自定义** 在 **Select your use case** 下
  - 单击**下一步：权限**
  - 在搜索框中，搜索并检查以下两个策略
	- **AmazonS3FullAccess**
	- **AWSGlueConsoleFullAccess**
  - 单击**下一步：标签**
	- 可选地添加标签，例如：
	  - workshop：AnalyticsOnAWS
  - 单击**下一步：审核**
  - 将**角色名称**指定为 `Analyticsworkshop_RedshiftRole`
  - 验证以下两个策略附加到角色：
	- **AmazonS3FullAccess**
	- **AWSGlueConsoleFullAccess**
  - 点击**创建角色**

> 在此处阅读有关将 IAM 角色与 Redshift 集群关联的更多信息：[https://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html](https://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html)

#### 创建 Redshift 集群

在此步骤中，您将创建一个 2 节点 Redshift 集群来创建 mini star 架构数据库。

  - 前往控制台：[https://console.aws.amazon.com/redshiftv2/home?region=us-east-1](https://console.aws.amazon.com/redshiftv2/home?region=us-east-1)
  - 单击**创建集群**
  - 将**集群标识符**保留为 **redshift-cluster-1**
  - 选择 **dc2.large** 作为**节点类型**
  - 选择节点数为 `2`
  - 验证**配置摘要**
  - 将**数据库端口（可选）**保留为 **5439**
  - 将**主用户名**更改为 `admin`
  - 输入**主用户密码**
    - 在选择密码之前，请检查主用户密码下面提到的密码规则（**此密码将用于访问集群**）
  - 扩展**集群权限（可选）**
    - 从下拉菜单中选择之前创建的 **Analyticsworkshop_RedshiftRole**
    - 单击**添加 IAM 角色**
    - **Analyticsworkshop_RedshiftRole** 应出现在**附加的 IAM 角色**下
  - 将**其他配置**保留为默认值。它使用默认 VPC 和默认安全组
  - 单击**创建集群**。集群将需要几分钟时间才能进入`可用`状态。**集群启动并处于可用状态后，移至下一步**
	


#### 创建 S3 网关端点

在这一步中，我们将创建 S3 网关端点，以便 Redshift 集群可以使用其私有 IP 与 S3 通信。

> 在此处阅读有关 Amazon Virtual Private Cloud (VPC) 的更多信息：[https://docs.amazonaws.cn/en_us/vpc/latest/userguide/what-is-amazon-vpc.html](https://docs.amazonaws.cn/en_us/vpc/latest/userguide/what-is-amazon-vpc.html)
>
> 在此处阅读有关 S3 网关端点的更多信息：[https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html)

- 前往 AWS VPC 控制台：[https://console.aws.amazon.com/vpc/home?region=us-east-1#Endpoints:sort=vpcEndpointId](https://console.aws.amazon.com/vpc/home?region=us-east-1#Endpoints:sort=vpcEndpointId)
- 单击**创建终端节点**
- 在**服务类别**下选择 **AWS 服务**（这是默认选择）	
- 在**服务名称**搜索框下，搜索“**s3**”并按回车/返回
- **com.amazonaws.us-east-1.s3** 应该作为搜索结果出现。选择此选项
- 在 **VPC** 下，选择默认 VPC。这与用于配置 redshift 集群的 VPC 相同
  - 如果下拉列表中列出了多个 VPC，请仔细检查 Redshift VPC 以避免任何混淆。请执行下列操作：
	- 前往 Redshift 控制台：[https://console.aws.amazon.com/redshiftv2/home?region=us-east-1](https://console.aws.amazon.com/redshiftv2/home?region=us-east-1)
	- 点击 **redshift-cluster-1**
	- 单击**属性**选项卡
	- 向下滚动并检查 VPC 名称的**网络和安全**部分
- 仔细检查 VPC id 后，转到**配置路由表**部分
- 选择列出的路由表（这应该是主路由表）。您可以通过在 **Main** 列下选中 **Yes** 来验证这一点

> 请注意针对配置路由表列出的 com.amazonaws.us-east-1.s3 id。它应该以 pl-[some number] 开头

  - 将**策略**保留为默认值（**FullAccess**）
  - 单击**创建终端节点**。配置应该需要几秒钟。准备就绪后，您应该会看到针对新创建的 S3 终端节点**可用**的状态
  - 编辑端点的名称为 `RedshiftS3EP`
	
#### 验证规则并将规则添加到默认安全组

在此步骤中，您将验证并向 Redshift 安全组添加规​​则，以便 Glue 服务可以与 Redshift 通信。

- 前往 VPC 安全组：[https://console.aws.amazon.com/vpc/home?region=us-east-1#SecurityGroups:sort=tag:Name](https://console.aws.amazon.com/vpc/home?region=us-east-1#SecurityGroups:sort=tag:Name)
- 选择 Redshift 安全组。如果在 Redshift 集群创建步骤期间未更改，则它应该是默认安全性
  - 如果列表中有多个安全组，请按照以下步骤操作：
	- 前往 Redshift 控制台：[https://console.aws.amazon.com/redshiftv2/home?region=us-east-1](https://console.aws.amazon.com/redshiftv2/home?region=us-east-1)
	- 点击 **redshift-cluster-1**
	- 单击**属性**选项卡。
	- 向下滚动并检查安全组 ID 的**网络和安全**部分
- 验证安全组后，选择安全组（行开头的复选框）。
- 单击**入站规则**。
  - 单击**编辑规则**
  - 检查是否存在自引用规则。 （默认情况下这应该可用，但如果没有，请添加如下所列的规则）
	- 类型：所有流量
	- 来源：[您正在编辑的同一安全组的名称] 注意：Glue 组件进行通信需要自引用规则
  - 为 Amazon S3 访问添加 **HTTPS** 规则。
	- 类型：`HTTPS`
	- 来源：[s3-prefix-list-id]
{{% notice note %}}
注意：在 **Source** 下选择 Custom 并在旁边的文本框中键入“pl”，之前创建的 S3 端点将显示并选择它
{{% /notice  %}}
![](/images/1.LabGuide/SecurityGroupsetting.png)
  - 单击**保存规则**。
- 单击**出站规则**。
  - 单击**编辑规则**
  - 保留现有规则，其具有以下值：
	- 类型：所有流量
	- 来源：`0.0.0.0/0`
  - 添加自引用规则。 （默认情况下这应该可用，但如果没有，请添加如下所列的规则）
	- 类型：所有 TCP
	- 来源：[您正在编辑的同一安全组的名称]
{{% notice note %}}
注意：Glue 组件进行通信需要一个自引用规则
{{% /notice  %}}
![](/images/1.LabGuide/SGOutboundrule.png)
  - 单击**保存规则**

> 在此处阅读有关为 Glue 访问设置 VPC 的更多信息：[https://docs.aws.amazon.com/glue/latest/dg/setup-vpc-for-glue-access.html](https://docs.aws.amazon.com/glue/latest/dg/setup-vpc-for-glue-access.html)

#### 在 Glue Connection 下创建 Redshift 连接。

在这一步中，我们将在 Glue 连接下创建一个 Redshift 连接，我们可以在 Development Endpoint 中使用它来建立与 Redshift 的连接。

- 前往 Glue Connections Console：[https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=connections](https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=connections)
  - 单击**添加连接**
	- 将**连接名称**指定为 `analytics_workshop`
	- 选择**连接类型**为 **Amazon Redshift**
	- 点击**下一步**
  - 设置对数据存储的访问权限：
	- 选择**集群**作为 **redshift-cluster-1**
	- 数据库名称和用户名应分别自动填充为 **dev** 和 **admin**
	- **密码**：输入您在 Redshift 集群设置期间使用的密码
	- 点击**下一步**
  - 查看连接详细信息，然后单击**完成**
  - 让我们测试连接：
	- 选择连接 **analytics_workshop**
	- 单击**测试连接**
	- 选择在之前**数据目录**模块中创建的 Glue 角色，即 **AnalyticsworkshopGlueRole**
	- 单击**测试连接**

此过程将需要一些时间。成功连接应显示以下消息：“**analytics_workshop** 已成功连接到您的实例。”

#### 创建数据结构和 Redshift 表。

- 前往 Redshift 查询编辑器：[https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#query-editor](https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#query-editor)
- 执行以下查询，为原始数据和参考数据创建架构和表
- 输入适当的数据库连接详细信息
- 点击**连接**

```
CREATE schema redshift_lab;
```

```
CREATE TABLE IF not EXISTS redshift_lab.f_raw_1 (
  uuid			varchar(256),
  device_ts 		timestamp,
  device_id		int,
  device_temp		int,
  track_id		int,
  activity_type		varchar(128),
  load_time		int
);
```

```
CREATE TABLE IF NOT EXISTS redshift_lab.d_ref_data_1 (
  track_id		int,
  track_name	varchar(128),
  artist_name	varchar(128)
);
```

#### 将数据转换并加载到 Redshift
##### 创建 Glue 开发端点

在此步骤中，您将创建一个 Glue Dev Endpoint 以使用 PySpark 以交互方式开发 Glue ETL 脚本

- 前往 Glue Development Endpoints：[https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=devEndpoints](https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=devEndpoints)
- 单击**添加端点**：
  - 开发端点名称：`analyticsworkshopEndpoint2`
	- IAM 角色：**AnalyticsworkshopGlueRole**
	- 展开**安全配置..参数**
	  - 数据处理单元 (DPU)：`2` 个（这降低了运行此实验室的成本）
	- 可选地添加标签，例如：
	  - workshop：AnalyticsOnAWS
  - 点击**下一步**
  - 网络画面：
	- 选择 **选择连接**
	- 选择 **analytics_workshop**
	- 点击**下一步**
  - 添加 SSH 公钥（可选）
	- 保留默认值
	- 点击**下一步**
  - 查看设置
	- 点击**完成**

新的 Glue 开发端点需要将近 10 分钟才能启动。

**您必须等待此步骤完成才能进入下一步。**

#### 为 Glue 开发端点创建 SageMaker Notebook (Jupyter)

- 前往 Notebooks 控制台：[https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=notebooks](https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=notebooks)
- 选择选项卡：**Sagemaker Notebooks**
- 单击**创建 notebook**
  - Notebook 名称：`AnalyticsworkshopRedshiftNotebook`
  - 附加到开发端点：**analyticsworkshopEndpoint2**
  - 选择**选择现有角色**
  - IAM 角色：**AnalyticsworkshopNotebookRole**
  - VPC（可选）：留空
  - 加密密钥（可选）：留空
  - 可选地添加标签，例如：
    - 研讨会：AnalyticsOnAWS
  - 点击：**创建 Notebook**

这将需要几分钟，请等待此操作完成。

#### 启动 Jupyter Notebook

- 在您的笔记本电脑本地下载并保存此文件：[analytics-workshop-redshift-notebook.ipynb](/files/analytics-workshop-redshift-notebook.ipynb)
- 前往 Notebooks 控制台：[https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=notebooks](https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=notebooks)
- 单击 **AnalyticsworkshopRedshiftNotebook**
- 单击**打开**以在新选项卡中打开 Notebook
- 在 Sagemaker Jupyter Notebook 中：
  - 单击**上传**（屏幕右上角）
  - 浏览并上传您之前下载的 **analytics-workshop-redshift-notebook.ipynb**
  - 点击**上传**确认
  - 点击 **analytics-workshop-redshift-notebook.ipynb** 打开笔记本
  - 确保它在笔记本的右上角显示“**Sparkmagic (PySpark)**”，这是 Jupyter 将用于在此笔记本中执行代码块的内核的名称
  - **按照笔记本上的说明进行操作**
  - 阅读并理解说明，它们解释了重要的 Glue 概念

#### 验证 - 转换/处理的数据已到达 Redshift

ETL 脚本成功运行后，转到 Redshift 查询编辑器：[https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#query-editor](https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#query-editor)

执行以下查询以检查原始数据表和参考数据表中的记录数。 

```
select count(1) from redshift_lab.f_raw_1;

select count(1) from redshift_lab.d_ref_data_1;
```

#### 可以尝试以下查询

```
select 
  track_name, 
  artist_name, 
  count(1) frequency
from 
  redshift_lab.f_raw_1 fr
inner join 
  redshift_lab.d_ref_data_1 drf
on 
  fr.track_id = drf.track_id
where 
  activity_type = 'Running'
group by 
  track_name, artist_name
order by 
  frequency desc
limit 10;
```

您已使用 Glue 无服务器 ETL 成功将数据从 S3 加载到 Redshift。

如果您想了解更多有关 Amazon Redshift 的信息，请按照以下可选部分进行操作。

{{% notice note %}}
！重要事项 如果是在自己的账户中进行试验，请务必在试验后清理创建的资源。
{{% /notice  %}}

## [可选部分]
#### Redshift 最佳实践

在此步骤中，您将实施一些 Redshift 最佳实践，以根据命中 Redshift 集群的查询来优化集群的存储和性能。上一节显示了来自数据消费的典型查询。

虽然您将在下面执行的大部分步骤在 Redshift 中都是自动化的，但了解关键概念及其工作方式总是好的。

- 使用 Redshift 查询编辑器在接下来的步骤中执行查询：
  - 前往 Redshift 查询编辑器：[https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#query-editor](https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#query-editor)

#### 字符串列大小

- 检查基于字符串的列的最大长度
  - 所有基于字符串的列的分配不应超过所需的精度。当您在查询中使用这些列时，它有助于减少集群上的存储空间，并通过消耗更少的内存来提高查询性能。 

```
select max(len(uuid)) as uuid_len, max(len(activity_type)) as activity_type_len from redshift_lab.f_raw_1;

select max(len(track_name)) as uuid_len, max(len(artist_name)) as activity_type_len from redshift_lab.d_ref_data_1;
```

- 上述查询的结果将用于确定基于字符串的列的新精度。
  - 通常，在定义列精度或列精度时维护缓冲区，如果它是列精度已知的标准属性，则可以固定。 
 
> 在此处阅读有关限制列大小的更多信息：[https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-smallest-column-size.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-smallest-column-size.html)

#### 压缩

在这一步中，我们将分析原始表以进行压缩。

- 检查每个表上的适当编码算法以实现压缩。
  - 压缩是一种列级操作，可在存储数据时减少数据的大小。 压缩可节省存储空间并减少从存储读取的数据大小，从而减少磁盘 I/O 量，从而提高查询性能。
  - 让我们关注事实表 f_raw_1 

```
analyze compression redshift_lab.f_raw_1;
```

上表的结果为每一列提供以下信息：

- 列
  - 编码：基于列精度类型和值的推荐编码。
  - Est_reduction_pct：如果实施推荐编码，则估计减少百分比。
{{% notice note %}}
注意：不要编码/压缩排序键。高度压缩的排序键意味着每个块有很多行，这反过来意味着扫描的块比需要的多。
{{% /notice  %}}

> 在此处阅读有关 Redshift 压缩的更多信息：[https://docs.aws.amazon.com/redshift/latest/dg/t_Compressing_data_on_disk.html](https://docs.aws.amazon.com/redshift/latest/dg/t_Compressing_data_on_disk.html)
>
> 在此处阅读有关在将数据加载到 Redshift 时自动启用压缩的更多信息：[https://docs.aws.amazon.com/redshift/latest/dg/c_Loading_tables_auto_compress.html](https://docs.aws.amazon.com/redshift/latest/dg/c_Loading_tables_auto_compress.html)

#### 分配方式

- 对事实表 f_raw_1 使用基于 KEY 的分布。
  - 由于两个表（f_raw_1 和 d_ref_data_1）需要在 track_id 列上连接，所以选择 track_id 作为分布键。
- 对维度表使用 ALL 分布 - d_ref_data_1。

> 在此处阅读有关 Redshift 分发样式最佳实践的更多信息：[https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html)

#### 排序键

在来自数据消费者的查询中，activity_type 属性已用于相等性过滤，因此可以选择该属性作为排序列。

> 在此处阅读有关 Redshift 排序键最佳实践的更多信息：[https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html)
>
> 在此处阅读有关 Amazon Redshift 最佳实践的更多信息：[https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html](https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html)

#### 新表 DDL

基于以上所有观察：

  - 使用下面的 f_raw_2 和 d_ref_data_2 的 DDL 创建一组新表： 

```
CREATE TABLE IF NOT EXISTS redshift_lab.f_raw_2 (
  uuid			varchar(40) ENCODE zstd,
  device_ts 		timestamp,
  device_id		int ENCODE az64,
  device_temp		int ENCODE az64,
  track_id		int ENCODE az64,
  activity_type		varchar(16) ENCODE bytedict sortkey,
  load_time		int ENCODE zstd
)
DISTKEY(track_id);
  
CREATE TABLE IF NOT EXISTS redshift_lab.d_ref_data_2 (
  track_id		int,
  track_name		varchar(64),
  artist_name		varchar(64)
)
DISTSTYLE ALL;
```

#### 将数据复制到新的一组表

由于 f_raw_1 和 d_ref_data_1 中的所有数据都已经存在，因此使用深拷贝将数据从旧表复制到新表。

-  执行以下 SQL 语句将数据从旧表复制到新表。 

```
insert into redshift_lab.f_raw_2 select * from redshift_lab.f_raw_1;
  
INSERT into redshift_lab.d_ref_data_2 select * from redshift_lab.d_ref_data_1; 
```

#### 运行 VACUUM 和 ANALYZE

在这一步中，我们将对表运行VACUUM和ANALYZE。

VACUUM：重新排序行并回收当前数据库中指定表或所有表中的空间

ANALYZE：更新供查询计划器使用的表统计信息 

```
vacuum redshift_lab.f_raw_2;
analyze redshift_lab.f_raw_2;  
vacuum redshift_lab.d_ref_data_2;
analyze redshift_lab.d_ref_data_2;
```

> 在此处阅读有关 Redshift VACUUM 的更多信息：[https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html](https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html)
>
> 在此处阅读有关 Redshift ANALYZE 的更多信息：[https://docs.aws.amazon.com/redshift/latest/dg/r_ANALYZE.html ](https://docs.aws.amazon.com/redshift/latest/dg/r_ANALYZE.html )

#### 执行查询

在新创建的表上重新执行数据使用者使用的以下查询。 

```
select 
  track_name, 
  artist_name, 
  count(1) frequency
from 
  redshift_lab.f_raw_2 fr
inner join 
  redshift_lab.d_ref_data_2 drf
on 
  fr.track_id = drf.track_id
where 
  activity_type = 'Running'
group by 
  track_name, artist_name
order by 
  frequency desc
limit 10;
```

如果您需要使用更多数据进行尝试，请从第一个模块 - 摄取和存储在 S3 存储桶中生成更多数据，然后从 Glue Notebook 再次加载数据。确保使用以下命令截断所有表中的现有数据： 

```
truncate table <schema name>.<table name>;
```

您刚刚学习并实施了一些 Redshift 最佳实践。

{{% notice note %}}
！重要事项 如果是在自己的账户中进行试验，请务必在试验后清理创建的资源。
{{% /notice  %}}